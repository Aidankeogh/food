{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a3789b4-c4f6-48e5-ad80-f3cb09b8c3b5",
   "metadata": {},
   "source": [
    "# Recipe Recommender Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e278673b-99e0-4d3b-b39b-3f2988d3161d",
   "metadata": {},
   "source": [
    "This runs the recommender models on the cleaned user-recipes interactions datasets and applies the various evaluation metrics for evaluating the results of our recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe085e-7fb3-497f-9281-974a3f365194",
   "metadata": {},
   "source": [
    "### Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca97642-c220-4abe-9715-c1d3ca144135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/spark-3.2.1/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-05-26 17:37:49,116 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Code from https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html\n",
    "import pyspark\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "conf = pyspark.SparkConf().setAll([('spark.master', 'local[2]'),\n",
    "                                   ('spark.app.name', 'Recommender Results')])\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12725ea-7ddb-4b52-961c-d1ccfe163988",
   "metadata": {},
   "source": [
    "### Read in cleaned user-recipes interactions data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9446bfa-c35f-4f42-bb8c-bcc126be3c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+\n",
      "|user_id|recipe_id|rating|\n",
      "+-------+---------+------+\n",
      "|  38094|    40893|     4|\n",
      "|1293707|    40893|     5|\n",
      "| 190375|   134728|     5|\n",
      "|1171894|   134728|     5|\n",
      "| 217118|   200236|     5|\n",
      "| 202555|   225241|     5|\n",
      "| 684460|   225241|     5|\n",
      "| 135017|   254596|     5|\n",
      "| 224088|   254596|     4|\n",
      "| 582223|   254596|     5|\n",
      "| 935485|   321038|     5|\n",
      "| 102602|    20930|     5|\n",
      "| 172467|    29093|     5|\n",
      "|  58332|    41090|     4|\n",
      "| 160497|    41090|     5|\n",
      "| 183565|    79222|     5|\n",
      "| 226989|    79222|     4|\n",
      "| 868654|    79222|     5|\n",
      "| 302867|    79222|     5|\n",
      "| 930021|    79222|     5|\n",
      "+-------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = 'file:///home/work/data/interactions_train_cleaned.csv'\n",
    "ratings = spark.read.csv(file_path, inferSchema = True, header = True)\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d39fcaf-827f-42ad-8d5f-d5924b56c8c3",
   "metadata": {},
   "source": [
    "### Random split and normalize training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9c1390e-70b8-43c4-b77e-70cf23005e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.670459606468806 0.7133942544421343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(unnorm_training, unnorm_test) = ratings.randomSplit([0.8, 0.2])\n",
    "mean = unnorm_training.agg({'rating': 'mean'}).collect()[0][0]\n",
    "std = unnorm_training.agg({'rating': 'std'}).collect()[0][0]\n",
    "print(mean, std)\n",
    "training = unnorm_training.withColumn(\"rating\", (col(\"rating\") - mean) / std)\n",
    "test = unnorm_test.withColumn(\"rating\", (col(\"rating\") - mean) / std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58b811-8483-45a3-a28a-4aa5e89b6d99",
   "metadata": {},
   "source": [
    "## (1) Modeling With the ALS Collaborative Filtering, pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920001a9-a524-4735-a88a-19e250450144",
   "metadata": {},
   "source": [
    "This was our first model that we tried out for building the recommender system.  It take the user-recipe rating data and build and ALS collaborative filtering model to predict the rating a user will give a recipe.  The predicted ratings can be used for choosing recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475250e7-69d3-4dec-b19e-707fd36c620c",
   "metadata": {},
   "source": [
    "### Generate recipe recommendations with the collaborative filtering model and evaluate with RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a5d8f5-90de-4c16-9ab7-fabcc31297a5",
   "metadata": {},
   "source": [
    "Fit collaborative filtering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bceeae7-ed5b-483d-b253-5a2a75c35175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 17:40:11,756 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "2022-05-26 17:40:11,760 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "2022-05-26 17:40:12,505 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Setting cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(rank=200, maxIter=20, regParam=0.125, userCol=\"user_id\", itemCol=\"recipe_id\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c8cbe8-9783-489c-8f71-22f3d1a0bb7d",
   "metadata": {},
   "source": [
    "Evaluate model with MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "298fc57d-289a-4e67-aafb-cc7c91ce4f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of the recommender model is 0.4734603465860362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 246:=======================================>                (7 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+------------------+\n",
      "|user_id|recipe_id|rating|        prediction|\n",
      "+-------+---------+------+------------------+\n",
      "| 269153|       81|   5.0| 4.733374731108822|\n",
      "| 194829|       91|   5.0| 4.768165041634855|\n",
      "| 851190|       92|   5.0| 4.842012929601356|\n",
      "|  79361|       93|   5.0|4.7234469979523865|\n",
      "| 983811|       93|   5.0| 4.282348661035171|\n",
      "| 531768|       94|   5.0| 4.712682822183593|\n",
      "|  20160|      112|   5.0|  4.35241798296385|\n",
      "| 719181|      112|   2.0| 4.726094430667179|\n",
      "| 315301|      142|   5.0| 4.648969488503619|\n",
      "| 513784|      142|   5.0| 4.725606861928464|\n",
      "|1386579|      142|   5.0|  4.74752338793877|\n",
      "| 175405|      175|   4.0| 4.705301482421126|\n",
      "| 603083|      185|   5.0| 4.578521558135922|\n",
      "|   1773|      190|   3.0| 5.024686632039058|\n",
      "| 177567|      190|   5.0| 4.694129449988864|\n",
      "|   3794|      191|   5.0|4.8129496635299365|\n",
      "| 680440|      192|   5.0| 4.770042689677924|\n",
      "| 862233|      192|   5.0| 4.688824982647968|\n",
      "|1537761|      192|   5.0| 4.767601438940255|\n",
      "|  91724|      197|   5.0| 4.661409712041955|\n",
      "+-------+---------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "normalized_predictions = model.transform(test)\n",
    "predictions = normalized_predictions.withColumn(\n",
    "    \"rating\",col(\"rating\") * std + mean\n",
    ").withColumn(\n",
    "    \"prediction\",col(\"prediction\") * std + mean\n",
    ")\n",
    "evaluator = RegressionEvaluator(metricName=\"mse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "\n",
    "mse = evaluator.evaluate(predictions)\n",
    "print(\"The MSE of the recommender model is\", mse)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980eaac0-e734-4f8f-a4c2-922660684683",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd2f498-5b55-4522-89b2-5ecd53eb5c85",
   "metadata": {},
   "source": [
    "We will load in the RecEvalMetrics object and thoroughly evaluate the recipe recommender system with various evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f0c8063-e655-4c74-b906-67878947c302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rbo\n",
      "  Downloading rbo-0.1.2-py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18 in /usr/local/lib/python3.8/dist-packages (from rbo) (1.21.0)\n",
      "Installing collected packages: rbo\n",
      "Successfully installed rbo-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6b8ae6a-244a-48e8-8903-ff88519cc2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rbo\n",
    "\n",
    "from functools import reduce\n",
    "from scipy.stats import kendalltau\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error, ndcg_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rbo\n",
    "\n",
    "class RecEvalMetrics(object):\n",
    "\n",
    "\n",
    "    # Takes user-recipe rating predictions dataframe, returns mean squarred error for top k\n",
    "    # recipes of each user predicted ratings\n",
    "    \"\"\" Parameters:\n",
    "        predictions: Dataframe of true and predicted ratings, default 20\n",
    "        k: Top k predicted ratings to evaluate with mse\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def top_k_evaluation(predictions, k = 20):\n",
    "        users = list(predictions.drop_duplicates(subset = ['user_id'])['user_id'])\n",
    "        top_k_predictions = []\n",
    "        \n",
    "        for user in users:\n",
    "            user_ratings = predictions[(predictions['user_id'] == user)]\n",
    "            top_k_user_ratings =  user_ratings.sort_values(by = ['prediction'], ascending = False).head(k)\n",
    "            top_k_predictions.append(top_k_user_ratings)\n",
    "        top_k_predictions_df = pd.concat(top_k_predictions, ignore_index = True)\n",
    "        \n",
    "        k_mse = mean_squared_error(list(top_k_predictions_df['rating']), list(top_k_predictions_df['prediction']))\n",
    "        \n",
    "        return(k_mse) \n",
    "\n",
    "\n",
    "    # Takes in user-recipe rating predictions dataframe, returns percent of recipes that ended\n",
    "    # up in someone's top k.  Larger value means more personalization\n",
    "    \"\"\" Parameters:\n",
    "        predictions: Dataframe of true and predicted ratings\n",
    "        k: Top k recipes to count in percentage, default 20\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def percent_in_top_ratings(predictions, k = 20):\n",
    "        total_recipes = len(predictions.drop_duplicates(subset = ['recipe_id']))\n",
    "        users = list(predictions.drop_duplicates(subset = ['user_id'])['user_id'])\n",
    "\n",
    "        top_k_predictions = set()\n",
    "        for user in users:\n",
    "            user_ratings = predictions[(predictions['user_id'] == user)]\n",
    "            user_pred_ordered = list(user_ratings.sort_values(by = ['prediction'], ascending = False)['recipe_id'])\n",
    "            top_k_user_recipes = user_pred_ordered[:k]\n",
    "            top_k_predictions.update(top_k_user_recipes)\n",
    "\n",
    "        top_recipes_count = len(top_k_predictions)\n",
    "\n",
    "        return(top_recipes_count/total_recipes)\n",
    "\n",
    "\n",
    "    # Take in user-recipe rating predictios dataframe, returns ranked biased overlap\n",
    "    # between top k predicted ratings and top k actual ratings\n",
    "    # Refer to: https://github.com/changyaochen/rbo\n",
    "    \"\"\" Parametes:\n",
    "        predictions: Dataframe of true and predicted ratings\n",
    "        k: Number of k recipes in the ranked list to evaluate with RBO, default 20\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def rbo_evaluation(predictions, k = 20):\n",
    "        users = list(predictions.drop_duplicates(subset = ['user_id'])['user_id'])\n",
    "\n",
    "        rbos = []\n",
    "        for user in users:\n",
    "            user_ratings = predictions[(predictions['user_id'] == user)]\n",
    "            user_actual_ordered = list(user_ratings.sort_values(by = ['rating'], ascending = False)['recipe_id'])\n",
    "            user_pred_ordered = list(user_ratings.sort_values(by = ['prediction'], ascending = False)['recipe_id'])\n",
    "            top_k_user_actual = user_actual_ordered[:k]\n",
    "            top_k_user_pred = user_pred_ordered[:k]\n",
    "            user_rbo = rbo.RankingSimilarity(top_k_user_actual, top_k_user_pred).rbo()\n",
    "            rbos.append(user_rbo)\n",
    "        \n",
    "        rbos_describe = pd.DataFrame(rbos)\n",
    "        return(rbos_describe.describe())\n",
    "\n",
    "\n",
    "    # Takes in user-recipe rating predictions dataframe, returns the Kendalls Tau evaluation\n",
    "    # between actual ratings and predicted ratings\n",
    "    \"\"\" Parameters:\n",
    "        predictions: Dataframe of true and predicted ratings\t\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def kendalls_tau(predictions):\n",
    "        users = list(predictions.drop_duplicates(subset = ['user_id'])['user_id'])\n",
    "\n",
    "        tau = []\n",
    "        i = 0\n",
    "        for user in users:\n",
    "            user_ratings = predictions[(predictions['user_id'] == user)]\n",
    "            # Kendall's Tau will not work with list of size 1\n",
    "            if (len(user_ratings) > 1):\n",
    "                user_actual_ordered = list(user_ratings.sort_values(by = ['rating'], ascending = False)['recipe_id'])\n",
    "                user_pred_ordered = list(user_ratings.sort_values(by = ['prediction'], ascending = False)['recipe_id'])\n",
    "                user_tau, user_p_value = kendalltau(user_actual_ordered, user_pred_ordered)\n",
    "                tau.append(user_tau)\n",
    "    \n",
    "        tau_describe = pd.DataFrame(tau)\n",
    "        return(tau_describe.describe())\n",
    "\n",
    "\n",
    "    # Takes in user-recipe rating predictions dataframe, returns the normalized discounted cummulative gain\n",
    "    # evaluation between actual ratings and predicted ratings\n",
    "    \"\"\" Parameters:\n",
    "        predictions: Dataframe of ture and predicted ratings\n",
    "        k: Number of k recipes in the ranked list to evaluate, default None\t\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def nDCG_evaluation(predictions, k = None):\n",
    "        users = list(predictions.drop_duplicates(subset = ['user_id'])['user_id'])\n",
    "\n",
    "        ndcg = []\n",
    "        for user in users:\n",
    "            user_ratings = predictions[(predictions['user_id'] == user)]\n",
    "            if (len(user_ratings) > 1):\n",
    "                relevance = np.asarray([list(user_ratings['rating'])])\n",
    "                preds = np.asarray([list(user_ratings['prediction'])])\n",
    "                score = ndcg_score(relevance, preds, k=k)\n",
    "                ndcg.append(score)\n",
    "        \n",
    "        ndcg_describe = pd.DataFrame(ndcg)\n",
    "        return(ndcg_describe.describe())\n",
    "    \n",
    "    # Takes in user-recipe rating predictions dataframe, returns the Spearman Rank Correlation\n",
    "    # evaluation between actual ratings and predicted ratings\n",
    "    \"\"\" Parameters:\n",
    "        predictions: Dataframe of ture and predicted ratings\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def spearman_evaluation(predictions, k = None):\n",
    "        users = list(predictions.drop_duplicates(subset = ['user_id'])['user_id'])\n",
    "\n",
    "        rhos = []\n",
    "        for user in users:\n",
    "            user_ratings = predictions[(predictions['user_id'] == user)]\n",
    "            if (len(user_ratings) > 1):\n",
    "                ratings = np.asarray(user_ratings['rating'])\n",
    "                preds = np.asarray(user_ratings['prediction'])\n",
    "                rho, pval = spearmanr(ratings, preds)\n",
    "                rhos.append(rho)\n",
    "        \n",
    "        rho_describe = pd.DataFrame(rhos)\n",
    "        return(rho_describe.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595de524-035e-4d09-b7e1-8b2b2678ee2f",
   "metadata": {},
   "source": [
    "Conver the predictions pyspark dataframe to a pandas dataframe for feeding into metrics evaluations. NOTE: We're doing this to keep the eval_metrics object generalized so that it could also accept dask results following conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55b8594c-ca91-4a22-9121-22c9b5e9e79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_df = predictions.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5385dc-04b0-4706-93e2-8a481f57b0fe",
   "metadata": {},
   "source": [
    "### Top 10 MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f93e6407-d2da-4218-b9e7-196da986e854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 MSE: 0.4750850644602118\n"
     ]
    }
   ],
   "source": [
    "k_mse = RecEvalMetrics.top_k_evaluation(predictions_df, 10)\n",
    "print(\"The top 10 MSE:\", k_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60e3b6-1c39-4fe9-96d4-8ea4b13c0859",
   "metadata": {},
   "source": [
    "### Percent in top 10 (Personalization Assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8f030be-36fa-4969-a11d-843275186214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percent of recipes that are in some users top 10: 0.9696149098004858\n"
     ]
    }
   ],
   "source": [
    "percent_in_top = RecEvalMetrics.percent_in_top_ratings(predictions_df, 10)\n",
    "print(\"The percent of recipes that are in some users top 10:\", percent_in_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f329733-f764-4545-b894-aa49e3433245",
   "metadata": {},
   "source": [
    "### Ranked Biased Overlap, top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20f86f4f-9195-451e-9559-047b3193cbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0\n",
      "count  19037.000000\n",
      "mean       0.790631\n",
      "std        0.236962\n",
      "min        0.108175\n",
      "25%        0.500000\n",
      "50%        1.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n"
     ]
    }
   ],
   "source": [
    "rbo_summary = RecEvalMetrics.rbo_evaluation(predictions_df, 10)\n",
    "print(rbo_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1adb5-0a5e-4388-bbba-a5c95b98eed1",
   "metadata": {},
   "source": [
    "### Kendall's Tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af09b974-bd94-44b5-a6ad-e125ff653470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0\n",
      "count  11834.000000\n",
      "mean       0.021153\n",
      "std        0.715416\n",
      "min       -1.000000\n",
      "25%       -0.466667\n",
      "50%        0.000000\n",
      "75%        0.666667\n",
      "max        1.000000\n"
     ]
    }
   ],
   "source": [
    "kendalls_tau_summary = RecEvalMetrics.kendalls_tau(predictions_df)\n",
    "print(kendalls_tau_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a285b-413a-4d62-85db-4bbac2e77d94",
   "metadata": {},
   "source": [
    "### Normalized Discounted Cummulative Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "538b9d9e-6c2c-4de0-b8c2-5654d354b21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0\n",
      "count  11834.000000\n",
      "mean       0.981945\n",
      "std        0.038286\n",
      "min        0.650069\n",
      "25%        0.983007\n",
      "50%        1.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n"
     ]
    }
   ],
   "source": [
    "ndcg_summary = RecEvalMetrics.nDCG_evaluation(predictions_df, 10)\n",
    "print(ndcg_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e544dc8e-dd61-4d3f-8803-861afc4c272e",
   "metadata": {},
   "source": [
    "### Spearman Rank Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b868e2f-bc4e-4f5e-9605-4eda68998b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/scipy/stats/_stats_py.py:4529: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(SpearmanRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0\n",
      "count  5710.000000\n",
      "mean      0.040791\n",
      "std       0.671445\n",
      "min      -1.000000\n",
      "25%      -0.500000\n",
      "50%       0.000000\n",
      "75%       0.670820\n",
      "max       1.000000\n"
     ]
    }
   ],
   "source": [
    "spearman_summary = RecEvalMetrics.spearman_evaluation(predictions_df)\n",
    "print(spearman_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3be5fb7-23a2-403d-97eb-7a4f62ed8815",
   "metadata": {},
   "source": [
    "## (2) Modeling with the Similarity Scorer and Averaging, dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a740208-9306-4758-8718-fcc0efd9776e",
   "metadata": {},
   "source": [
    "This uses an alternative approach to predicting user ratings through similarity scoring metrics and different types of rating averaging.  This part is done in dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "902d2e33-20b7-464b-94c0-7fa492f98f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sparse\n",
      "  Downloading sparse-0.13.0-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 6.1 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numba>=0.49 in /usr/local/lib/python3.8/dist-packages (from sparse) (0.55.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from sparse) (1.21.0)\n",
      "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.8/dist-packages (from sparse) (1.8.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->sparse) (0.38.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from numba>=0.49->sparse) (45.2.0)\n",
      "Installing collected packages: sparse\n",
      "Successfully installed sparse-0.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c87ec0-6fee-4c93-9d68-36cae314fafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 00:20:47,486 - distributed.diskutils - INFO - Found stale lock file and directory '/home/work/dask-worker-space/worker-1k2guxok', purging\n",
      "2022-05-26 00:20:47,496 - distributed.diskutils - INFO - Found stale lock file and directory '/home/work/dask-worker-space/worker-ab10wnq0', purging\n",
      "2022-05-26 00:20:47,509 - distributed.diskutils - INFO - Found stale lock file and directory '/home/work/dask-worker-space/worker-k8nzx1qj', purging\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import sparse\n",
    "import dask_ml\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(memory_limit='6GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce05280-3b58-4acc-b646-9b280b4e066d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308015 34001\n"
     ]
    }
   ],
   "source": [
    "seed = 25\n",
    "ddf = dd.read_csv(\"data/interactions_train_cleaned.csv\")\n",
    "train, val = dask_ml.model_selection.train_test_split(ddf, test_size=0.1, train_size=0.9,shuffle=True,random_state=seed)\n",
    "print(len(train), len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a1885c-4aef-469a-8359-b3e0c604f8e2",
   "metadata": {},
   "source": [
    "### Using Averages as Predicted Ratings:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf6eab-39e5-40ea-96d1-fe5f5a77d267",
   "metadata": {},
   "source": [
    "Computing rating average and rating standard deviation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5b7672-8166-4ed9-ae4d-0422b28f9375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.671889356037855 0.7121078429179404\n"
     ]
    }
   ],
   "source": [
    "rating_avg = train.rating.mean().compute()\n",
    "rating_std = train.rating.std().compute()\n",
    "print(rating_avg, rating_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f70e5-ef70-4660-b8c0-fb92111a9f17",
   "metadata": {},
   "source": [
    "### Baseline 1: Predict using global average rating among all users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c45f80-9b78-46e9-ba5d-79d71f3f58ee",
   "metadata": {},
   "source": [
    "How well would a model perform with just rating averages as predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "023f8425-0435-476c-831f-d346b9e82114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5169555594737777\n"
     ]
    }
   ],
   "source": [
    "val[\"prediction\"] = rating_avg\n",
    "print(dask_ml.metrics.mean_squared_error(val.rating.to_dask_array(), val.prediction.to_dask_array()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e5f70-8092-4a73-a09c-cd8bb9e903b8",
   "metadata": {},
   "source": [
    "### Baseline 2: Predict using average rating for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c6c0160-4186-403a-8eb9-8f9f45e8c5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4964676076844779\n"
     ]
    }
   ],
   "source": [
    "user_avgs = train.groupby(\"user_id\").rating.mean().compute()\n",
    "val[\"prediction\"] = val.user_id.apply(\n",
    "    lambda x: user_avgs[x] if x in user_avgs else rating_avg, \n",
    "    meta=('user_id', 'int64')\n",
    ")\n",
    "print(dask_ml.metrics.mean_squared_error(val.rating.to_dask_array(), val.prediction.to_dask_array()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cbf648-a754-4e80-8260-2263d6ea095c",
   "metadata": {},
   "source": [
    "### Baseline 3: Predict using average rating for each user, bayesian style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e59adc4-9380-42f4-9201-36c3ffe55e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     k       err\n",
      "0  6.0  0.470208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_398/612394044.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bayesian_df = bayesian_df.append({\"k\": k, \"err\": err}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "bayesian_df = pd.DataFrame()\n",
    "user_avgs = train.groupby(\"user_id\").rating.mean().compute()\n",
    "user_counts = train.groupby(\"user_id\").rating.count().compute()\n",
    "k = 6\n",
    "val[\"personal_rating\"] = val.user_id.apply(\n",
    "    lambda x: (rating_avg * k + user_avgs[x] * user_counts[x]) / (user_counts[x] + k) if x in user_avgs else rating_avg, \n",
    "    meta=('personal_rating', 'float32')\n",
    ")\n",
    "err = dask_ml.metrics.mean_squared_error(val.rating.to_dask_array(), val.personal_rating.to_dask_array())\n",
    "bayesian_df = bayesian_df.append({\"k\": k, \"err\": err}, ignore_index=True)\n",
    "print(bayesian_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74d290-9bcb-4d22-81c4-3e3ac03ab079",
   "metadata": {},
   "source": [
    "### Similarity Scorer Recommender Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9130a2d2-c931-4242-87e8-ab941bcd1519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating indices\n",
      "Creating sparse matrix 12.197404861450195\n",
      "Generating dot products 20.847816228866577\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'COO' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/array/reductions.py:346\u001b[0m, in \u001b[0;36mpartial_reduce\u001b[0;34m(func, x, split_every, keepdims, dtype, name, reduced_meta)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 346\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduced_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputing_meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# no meta keyword argument exists for func, and it isn't required\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: _chunk_sum() got an unexpected keyword argument 'computing_meta'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m         predicted_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarities[user_idx] \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_mat[:, recipe_idx]\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m predicted_score\n\u001b[0;32m---> 52\u001b[0m scorer \u001b[38;5;241m=\u001b[39m \u001b[43mSimilarityScorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mSimilarityScorer.__init__\u001b[0;34m(self, interactions)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_mat \u001b[38;5;241m=\u001b[39m da\u001b[38;5;241m.\u001b[39mfrom_array(s, chunks\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5000\u001b[39m, \u001b[38;5;241m5000\u001b[39m))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating dot products\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n\u001b[0;32m---> 26\u001b[0m dot_product_similarities \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_mat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_mat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m)\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw similarities computed\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n\u001b[1;32m     29\u001b[0m dense_similarities \u001b[38;5;241m=\u001b[39m dot_product_similarities\u001b[38;5;241m.\u001b[39mtodense()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/array/core.py:218\u001b[0m, in \u001b[0;36mcheck_if_handled_given_other.<locals>.wrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/array/core.py:2301\u001b[0m, in \u001b[0;36mArray.__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   2297\u001b[0m \u001b[38;5;129m@check_if_handled_given_other\u001b[39m\n\u001b[1;32m   2298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__matmul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m   2299\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroutines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m matmul\n\u001b[0;32m-> 2301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/array/routines.py:464\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    446\u001b[0m out \u001b[38;5;241m=\u001b[39m blockwise(\n\u001b[1;32m    447\u001b[0m     _matmul,\n\u001b[1;32m    448\u001b[0m     out_ind,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m     concatenate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    456\u001b[0m )\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# Because contraction + concatenate in blockwise leads to high\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;66;03m# memory footprints, we want to avoid them. Instead we will perform\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# blockwise (without contraction) followed by reduction. More about\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# this issue: https://github.com/dask/dask/issues/6874\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# We will also perform the reduction without concatenation\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43m_sum_wo_cat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a_is_1d:\n\u001b[1;32m    467\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/array/routines.py:384\u001b[0m, in \u001b[0;36m_sum_wo_cat\u001b[0;34m(a, axis, dtype)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mshape[axis] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\u001b[38;5;241m.\u001b[39msqueeze(axis)\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_sum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_sum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/array/reductions.py:218\u001b[0m, in \u001b[0;36mreduction\u001b[0;34m(x, chunk, aggregate, axis, keepdims, dtype, split_every, combine, name, out, concatenate, output_size, meta, weights)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     reduced_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_tree_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduced_meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduced_meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mand\u001b[39;00m output_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    231\u001b[0m     result\u001b[38;5;241m.\u001b[39m_chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    232\u001b[0m         (output_size,) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m axis \u001b[38;5;28;01melse\u001b[39;00m c \u001b[38;5;28;01mfor\u001b[39;00m i, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tmp\u001b[38;5;241m.\u001b[39mchunks)\n\u001b[1;32m    233\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/array/reductions.py:286\u001b[0m, in \u001b[0;36m_tree_reduce\u001b[0;34m(x, aggregate, axis, keepdims, dtype, split_every, combine, name, concatenate, reduced_meta)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concatenate:\n\u001b[1;32m    285\u001b[0m     func \u001b[38;5;241m=\u001b[39m compose(func, partial(_concatenate2, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msorted\u001b[39m(axis)))\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartial_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuncname\u001b[49m\u001b[43m(\u001b[49m\u001b[43maggregate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-aggregate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduced_meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduced_meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/array/reductions.py:350\u001b[0m, in \u001b[0;36mpartial_reduce\u001b[0;34m(func, x, split_every, keepdims, dtype, name, reduced_meta)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m         meta \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduced_meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;66;03m# min/max functions have no identity, don't apply function to meta\u001b[39;00m\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero-size array to reduction operation\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/array/routines.py:374\u001b[0m, in \u001b[0;36m_chunk_sum\u001b[0;34m(a, axis, dtype, keepdims)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m(axis[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'COO' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "train[\"personal_rating\"] = train.user_id.apply(\n",
    "    lambda x: (rating_avg * k + user_avgs[x] * user_counts[x]) / (user_counts[x] + k) if x in user_avgs else rating_avg, \n",
    "    meta=('personal_rating', 'float32')\n",
    ")\n",
    "train[\"person_normalized_rating\"] = train.rating - train.personal_rating\n",
    "\n",
    "class SimilarityScorer:\n",
    "    def __init__(self, interactions):\n",
    "        start = time.time()\n",
    "        print(\"Generating indices\")\n",
    "        user_codes, self.user_idx_to_id = pd.factorize(interactions.user_id.compute())\n",
    "        recipe_codes, self.recipe_idx_to_id = pd.factorize(interactions.recipe_id.compute())\n",
    "        self.user_id_to_idx = {user: idx for idx, user in enumerate(self.user_idx_to_id)}\n",
    "        self.recipe_id_to_idx = {recipe: idx for idx, recipe in enumerate(self.recipe_idx_to_id)}\n",
    "        \n",
    "        print(\"Creating sparse matrix\", time.time() - start)\n",
    "        s = sparse.COO(\n",
    "            [user_codes, recipe_codes],\n",
    "            interactions.person_normalized_rating.compute(),\n",
    "            shape=(len(self.user_idx_to_id), len(self.recipe_idx_to_id)),\n",
    "            fill_value=0\n",
    "        )\n",
    "        self.sparse_mat = da.from_array(s, chunks=(5000, 5000))\n",
    "        print(\"Generating dot products\", time.time() - start)\n",
    "\n",
    "        dot_product_similarities = (self.sparse_mat @ self.sparse_mat.T).compute()\n",
    "        print(\"Raw similarities computed\", time.time() - start)\n",
    "        \n",
    "        dense_similarities = dot_product_similarities.todense()\n",
    "        sims_summed = dense_similarities.sum(axis=1) + 1e-20\n",
    "        self.similarities = dense_similarities / sims_summed.reshape(-1, 1)\n",
    "        self.sparse_mat = self.sparse_mat.compute()\n",
    "        print(\"Similarities normalized!\", time.time() - start)\n",
    "\n",
    "    def predict_topk_for_user(self, user_id, k):\n",
    "        user_idx = self.user_id_to_idx[user_id]\n",
    "        similarities_norm = self.similarities_normalized[user_idx]\n",
    "        recs = self.similarities[user_idx] @ self.sparse_mat\n",
    "        rec_values = recs.topk(k)\n",
    "        rec_idxs = recs.argtopk(k)\n",
    "        recs_ids = [self.recipe_idx_to_id[idx] for idx in rec_idxs]\n",
    "        return recs_ids, rec_values\n",
    "    \n",
    "    def predict_pair(self, user_id, recipe_id):\n",
    "        if recipe_id not in self.recipe_id_to_idx or user_id not in self.user_id_to_idx:\n",
    "            return 0\n",
    "        user_idx = self.user_id_to_idx[user_id]\n",
    "        recipe_idx = self.recipe_id_to_idx[recipe_id]\n",
    "        predicted_score = self.similarities[user_idx] @ self.sparse_mat[:, recipe_idx]\n",
    "        return predicted_score\n",
    "\n",
    "scorer = SimilarityScorer(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aba48ce-13a8-497d-a7ae-3f5e81b12628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6b11d-4b16-4b5a-8064-e7756f862a21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
