{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a3789b4-c4f6-48e5-ad80-f3cb09b8c3b5",
   "metadata": {},
   "source": [
    "# Recipe Recommender Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e278673b-99e0-4d3b-b39b-3f2988d3161d",
   "metadata": {},
   "source": [
    "This notebooks runs the recommender models on the cleaned user-recipes interactions datasets and applies the various evaluation metrics for evaluating the results of our recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe085e-7fb3-497f-9281-974a3f365194",
   "metadata": {},
   "source": [
    "### Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fca97642-c220-4abe-9715-c1d3ca144135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html\n",
    "import pyspark\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "conf = pyspark.SparkConf().setAll([('spark.master', 'local[2]'),\n",
    "                                   ('spark.app.name', 'Recommender Results')])\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12725ea-7ddb-4b52-961c-d1ccfe163988",
   "metadata": {},
   "source": [
    "### Read in cleaned user-recipes interactions data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9446bfa-c35f-4f42-bb8c-bcc126be3c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+\n",
      "|user_id|recipe_id|rating|\n",
      "+-------+---------+------+\n",
      "|  38094|    40893|     4|\n",
      "|1293707|    40893|     5|\n",
      "| 190375|   134728|     5|\n",
      "|1171894|   134728|     5|\n",
      "| 217118|   200236|     5|\n",
      "| 202555|   225241|     5|\n",
      "| 684460|   225241|     5|\n",
      "| 135017|   254596|     5|\n",
      "| 224088|   254596|     4|\n",
      "| 582223|   254596|     5|\n",
      "| 935485|   321038|     5|\n",
      "| 102602|    20930|     5|\n",
      "| 172467|    29093|     5|\n",
      "|  58332|    41090|     4|\n",
      "| 160497|    41090|     5|\n",
      "| 183565|    79222|     5|\n",
      "| 226989|    79222|     4|\n",
      "| 868654|    79222|     5|\n",
      "| 302867|    79222|     5|\n",
      "| 930021|    79222|     5|\n",
      "+-------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = 'file:///home/work/data/interactions_train_cleaned.csv'\n",
    "ratings = spark.read.csv(file_path, inferSchema = True, header = True)\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d39fcaf-827f-42ad-8d5f-d5924b56c8c3",
   "metadata": {},
   "source": [
    "### Random split and normalize training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9c1390e-70b8-43c4-b77e-70cf23005e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.646661593045566 0.7330842167730709\n"
     ]
    }
   ],
   "source": [
    "(unnorm_training, unnorm_test) = ratings.randomSplit([0.8, 0.2])\n",
    "mean = unnorm_training.agg({'rating': 'mean'}).collect()[0][0]\n",
    "std = unnorm_training.agg({'rating': 'std'}).collect()[0][0]\n",
    "print(mean, std)\n",
    "training = unnorm_training.withColumn(\"rating\", (col(\"rating\") - mean) / std)\n",
    "test = unnorm_test.withColumn(\"rating\", (col(\"rating\") - mean) / std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58b811-8483-45a3-a28a-4aa5e89b6d99",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475250e7-69d3-4dec-b19e-707fd36c620c",
   "metadata": {},
   "source": [
    "### Generate recipe recommendations with the collaborative filtering model and evaluate with RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a5d8f5-90de-4c16-9ab7-fabcc31297a5",
   "metadata": {},
   "source": [
    "Fit collaborative filtering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bceeae7-ed5b-483d-b253-5a2a75c35175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 18:39:04,510 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "2022-05-25 18:39:04,514 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "2022-05-25 18:39:05,072 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Setting cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(rank=200, maxIter=20, regParam=0.125, userCol=\"user_id\", itemCol=\"recipe_id\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c8cbe8-9783-489c-8f71-22f3d1a0bb7d",
   "metadata": {},
   "source": [
    "Evaluate model with MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "298fc57d-289a-4e67-aafb-cc7c91ce4f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of the recommender model is 0.4976449210815721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 263:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+------------------+\n",
      "|user_id|recipe_id|rating|        prediction|\n",
      "+-------+---------+------+------------------+\n",
      "| 132353|       40|   5.0| 4.655222910317792|\n",
      "| 194829|       91|   5.0| 4.732509125103519|\n",
      "| 709476|      142|   5.0| 4.628799670807802|\n",
      "|  99979|      142|   5.0| 4.673064912995164|\n",
      "|  10649|      190|   3.0|4.9240342017110486|\n",
      "|  91326|      192|   4.0| 4.691164935972212|\n",
      "| 143387|      192|   5.0|4.6850852097682845|\n",
      "| 145599|      192|   5.0|  4.67063368067291|\n",
      "| 151679|      192|   5.0| 4.689854071049422|\n",
      "| 675287|      231|   5.0| 4.777932053142876|\n",
      "| 156951|      232|   2.0| 4.724549941677335|\n",
      "| 625864|      232|   5.0| 4.526215903805111|\n",
      "| 983634|      232|   5.0| 4.651344504796338|\n",
      "|  84361|      251|   3.0|4.7239619842006375|\n",
      "| 111075|      271|   5.0| 4.715486405151405|\n",
      "| 120448|      280|   5.0| 4.681396177642237|\n",
      "| 354878|      310|   5.0| 4.714933458473203|\n",
      "| 249059|      324|   5.0|  4.64725394995715|\n",
      "| 275316|      324|   5.0| 4.657538708050218|\n",
      "| 591403|      346|   5.0|  4.68704871475454|\n",
      "+-------+---------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "normalized_predictions = model.transform(test)\n",
    "predictions = normalized_predictions.withColumn(\n",
    "    \"rating\",col(\"rating\") * std + mean\n",
    ").withColumn(\n",
    "    \"prediction\",col(\"prediction\") * std + mean\n",
    ")\n",
    "evaluator = RegressionEvaluator(metricName=\"mse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "\n",
    "mse = evaluator.evaluate(predictions)\n",
    "print(\"The MSE of the recommender model is\", mse)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980eaac0-e734-4f8f-a4c2-922660684683",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd2f498-5b55-4522-89b2-5ecd53eb5c85",
   "metadata": {},
   "source": [
    "We will load in the RecEvalMetrics object and thoroughly evaluate the recipe recommender system with various evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d6b8ae6a-244a-48e8-8903-ff88519cc2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rbo\n",
    "# !pip install recmetrics\n",
    "\n",
    "from functools import reduce\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.metrics import mean_squared_error, ndcg_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import recmetrics\n",
    "import rbo\n",
    "\n",
    "class RecEvalMetrics(object):\n",
    "\n",
    "\n",
    "    # Takes user-recipe rating predictions dataframe, returns mean squarred error for top k\n",
    "    # recipes of each user predicted ratings\n",
    "    \"\"\" Parameters:\n",
    "        predictions: Dataframe of true and predicted ratings, default 20\n",
    "        k: Top k predicted ratings to evaluate with mse\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def top_k_evaluator(predictions, k = 20):\n",
    "        users = list(predictions.drop_duplicates(subset = ['user_id'])['user_id'])\n",
    "        top_k_predictions = []\n",
    "        \n",
    "        for user in users:\n",
    "            user_ratings = predictions[(predictions['user_id'] == user)]\n",
    "            top_k_user_ratings =  user_ratings.sort_values(by = ['prediction'], ascending = False).head(k)\n",
    "            top_k_predictions.append(top_k_user_ratings)\n",
    "        top_k_predictions_df = pd.concat(top_k_predictions, ignore_index = True)\n",
    "        \n",
    "        k_mse = mean_squared_error(list(top_k_predictions_df['rating']), list(top_k_predictions_df['prediction']))\n",
    "        \n",
    "        return(k_mse) \n",
    "\n",
    "\n",
    "    # Takes in user-recipe rating predictions dataframe, returns percent of recipes that ended\n",
    "    # up in someone's top k.  Larger value means more personalization\n",
    "    \"\"\" Parameters:\n",
    "        predictions: Dataframe of true and predicted ratings\n",
    "        k: Top k recipes to count in percentage, default 20\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def percent_in_top_ratings(predictions, k = 20):\n",
    "        total_recipes = len(predictions.drop_duplicates(subset = ['recipe_id']))\n",
    "        users = list(predictions.drop_duplicates(subset = ['user_id'])['user_id'])\n",
    "\n",
    "        top_k_predictions = set()\n",
    "        for user in users:\n",
    "            user_ratings = predictions[(predictions['user_id'] == user)]\n",
    "            user_pred_ordered = list(user_ratings.sort_values(by = ['prediction'], ascending = False)['recipe_id'])\n",
    "            top_k_user_recipes = user_pred_ordered[:k]\n",
    "            top_k_predictions.update(top_k_user_recipes)\n",
    "\n",
    "        top_recipes_count = len(top_k_predictions)\n",
    "\n",
    "        return(top_recipes_count/total_recipes)\n",
    "\n",
    "\n",
    "    # Take in user-recipe rating predictios dataframe, returns ranked biased overlap\n",
    "    # between top k predicted ratings and top k actual ratings\n",
    "    # Refer to: https://github.com/changyaochen/rbo\n",
    "    \"\"\" Parametes:\n",
    "        predictions: Dataframe of true and predicted ratings\n",
    "        k: Number of k recipes in the ranked list to evaluate with RBO, default 20\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def rbo_evaluation(predictions, k = 20):\n",
    "        users = list(predictions.drop_duplicates(subset = ['user_id'])['user_id'])\n",
    "\n",
    "        rbos = []\n",
    "        for user in users:\n",
    "            user_ratings = predictions[(predictions['user_id'] == user)]\n",
    "            user_actual_ordered = list(user_ratings.sort_values(by = ['rating'], ascending = False)['recipe_id'])\n",
    "            user_pred_ordered = list(user_ratings.sort_values(by = ['prediction'], ascending = False)['recipe_id'])\n",
    "            top_k_user_actual = user_actual_ordered[:k]\n",
    "            top_k_user_pred = user_pred_ordered[:k]\n",
    "            user_rbo = rbo.RankingSimilarity(top_k_user_actual, top_k_user_pred).rbo()\n",
    "            rbos.append(user_rbo)\n",
    "            \n",
    "        return(np.mean(rbos), np.median(rbos))\n",
    "\n",
    "\n",
    "    # Takes in user-recipe rating predictions dataframe, returns the Kendalls Tau evaluation\n",
    "    # between actual ratings and predicted ratings\n",
    "    \"\"\" Parameters:\n",
    "        predictions: Dataframe of true and predicted ratings\t\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def kendalls_tau(predictions):\n",
    "        users = list(predictions.drop_duplicates(subset = ['user_id'])['user_id'])\n",
    "\n",
    "        tau = []\n",
    "        i = 0\n",
    "        for user in users:\n",
    "            user_ratings = predictions[(predictions['user_id'] == user)]\n",
    "            # Kendall's Tau will not work with list of size 1\n",
    "            if (len(user_ratings) > 1):\n",
    "                user_actual_ordered = list(user_ratings.sort_values(by = ['rating'], ascending = False)['recipe_id'])\n",
    "                user_pred_ordered = list(user_ratings.sort_values(by = ['prediction'], ascending = False)['recipe_id'])\n",
    "                user_tau, user_p_value = kendalltau(user_actual_ordered, user_pred_ordered)\n",
    "                tau.append(user_tau)\n",
    "\n",
    "        return(np.mean(tau), np.median(tau))\n",
    "\n",
    "\n",
    "    # Takes in user-recipe rating predictions dataframe, returns the normalized discounted cummulative gain\n",
    "    # evaluation between actual ratings and predicted ratings\n",
    "    \"\"\" Parameters:\n",
    "        predictions: Dataframe of ture and predicted ratings\n",
    "    k: Number of k recipes in the ranked list to evaluate, default None\t\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def nDCG_evaluation(predictions, k = None):\n",
    "        users = list(predictions.drop_duplicates(subset = ['user_id'])['user_id'])\n",
    "\n",
    "        ndcg = []\n",
    "        i = 0\n",
    "        for user in users:\n",
    "            user_ratings = predictions[(predictions['user_id'] == user)]\n",
    "            if (len(user_ratings) > 1):\n",
    "                relevance = np.asarray([list(user_ratings['rating'])])\n",
    "                preds = np.asarray([list(user_ratings['prediction'])])\n",
    "                score = ndcg_score(relevance, preds, k=k)\n",
    "                ndcg.append(score)\n",
    "\n",
    "        return(np.mean(ndcg), np.median(ndcg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595de524-035e-4d09-b7e1-8b2b2678ee2f",
   "metadata": {},
   "source": [
    "Conver the predictions pyspark dataframe to a pandas dataframe for feeding into metrics evaluations. NOTE: We're doing this to keep the eval_metrics object generalized so that it could also accept dask results following conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55b8594c-ca91-4a22-9121-22c9b5e9e79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_df = predictions.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5385dc-04b0-4706-93e2-8a481f57b0fe",
   "metadata": {},
   "source": [
    "### Top 10 MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f93e6407-d2da-4218-b9e7-196da986e854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 MSE: 0.4973753393177582\n"
     ]
    }
   ],
   "source": [
    "k_mse = RecEvalMetrics.top_k_evaluator(predictions_df, 10)\n",
    "print(\"The top 10 MSE:\", k_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60e3b6-1c39-4fe9-96d4-8ea4b13c0859",
   "metadata": {},
   "source": [
    "### Percent in top 10 (Personalization Assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8f030be-36fa-4969-a11d-843275186214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percent of recipes that are in some users top 10: 0.98972338785647\n"
     ]
    }
   ],
   "source": [
    "percent_in_top = RecEvalMetrics.percent_in_top_ratings(predictions_df, 10)\n",
    "print(\"The percent of recipes that are in some users top 10:\", percent_in_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f329733-f764-4545-b894-aa49e3433245",
   "metadata": {},
   "source": [
    "### Ranked Biased Overlap, top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20f86f4f-9195-451e-9559-047b3193cbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RBO mean for all user recommendations: 0.8288452577530352\n",
      "The RBO median for all user recommendations: 1.0\n"
     ]
    }
   ],
   "source": [
    "rbo_mean, rbo_median = RecEvalMetrics.rbo_evaluation(predictions_df, 10)\n",
    "print(\"The RBO mean for all user recommendations:\", rbo_mean)\n",
    "print(\"The RBO median for all user recommendations:\", rbo_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1adb5-0a5e-4388-bbba-a5c95b98eed1",
   "metadata": {},
   "source": [
    "### Kendall's Tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af09b974-bd94-44b5-a6ad-e125ff653470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Kendall's Tau mean for all user recommendations: 0.015742770758176938\n",
      "The Kendall's Tau median for all user recommendations: 0.0\n"
     ]
    }
   ],
   "source": [
    "tau_mean, tau_median = RecEvalMetrics.kendalls_tau(predictions_df)\n",
    "print(\"The Kendall's Tau mean for all user recommendations:\", tau_mean)\n",
    "print(\"The Kendall's Tau median for all user recommendations:\", tau_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a285b-413a-4d62-85db-4bbac2e77d94",
   "metadata": {},
   "source": [
    "### Normalized Discounted Cummulative Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "538b9d9e-6c2c-4de0-b8c2-5654d354b21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nDCG mean for all user recommendations: 0.9820059066538368\n",
      "the nDCG median for all user recommendations: 1.0\n"
     ]
    }
   ],
   "source": [
    "ndcg_mean, ndcg_median = RecEvalMetrics.nDCG_evaluation(predictions_df, 10)\n",
    "print(\"The nDCG mean for all user recommendations:\", ndcg_mean)\n",
    "print(\"the nDCG median for all user recommendations:\", ndcg_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed50b175-f3ca-4f5f-8adf-54bfda3e2dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
